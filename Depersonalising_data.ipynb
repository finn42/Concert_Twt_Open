{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTS concert twt data depersonalisation \n",
    "\n",
    "This notebook is transforms tweet dataset collected with the Twitter Streaming API into minimal subsets for the open sharing of essential information. \n",
    "\n",
    "Tweets were collected during four online broadcast BTS concerts in 2021. Streaming APIs capture tweets at the time of posting according to predefined monitoring criteria made up of user ids, keywords, and hashtags (Twitter Developer Platform, 2021). All public tweets were captured in the interval of streaming (~4 hrs each) accroding to pre-established criteria, except in cases where rate limits interfered. The tweet data was collected and stored by the Center for an Informed Public at the University of Washington.  For each tweet capture, the API logged what and when the status update was posted, along with information on tweets related by retweet, quote tweet, and reply, and account details for the posting user and users of related tweets such as their user id, number of followers, and account language.\n",
    "\n",
    "According to the API research agreement, CIPs best practices, and out of respect to the privacy of the users sampled, we cannot publish the full original datasets. As a compromise, this notebook records how two subsets were generated from the full records to make accessible the minimum information behind the analyses presented in \"Audience Reconstructed\". \n",
    "\n",
    "## Filter Tweets per request\n",
    "Tweets from initial samplings are filtered first to remove tweets and retweets of accounts that specify a request to be excluded from off-platform uses. \n",
    "\n",
    "\n",
    "## Filter Tweets of Official Accounts\n",
    "Tweets by official accounts get attention in different patterns than tweets by fans, and the dynamics can overwhile the interactions of interest. Here we take out tweets pertaining to (reply to, RT of) official accounts related to BTS.\n",
    "\n",
    "## Depersonalisation of full tweet sets\n",
    "Share full set of filtered recording tweets after depersonalising entries by dropping unused fields, replacing potentially identifying fields with obscuring features, and replacing identifying id numbers such as user IDs and Tweets IDs.\n",
    "\n",
    "## Tweet datasets for content analysis.\n",
    "A sampling of 1200 tweets from these concert tweet datasets were evaluated for content. A subset of fields are the content codes are retained for publication.\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import math\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import respy functions from twt.py file\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport twt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data_loc.txt','r')\n",
    "raw_dir = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>raw_loc</th>\n",
       "      <th>fullfeilds_loc</th>\n",
       "      <th>dep_loc</th>\n",
       "      <th>raw_twt_db</th>\n",
       "      <th>full_twt_db</th>\n",
       "      <th>fan_twt_db</th>\n",
       "      <th>dep_twt_db</th>\n",
       "      <th>event_file</th>\n",
       "      <th>event_offset</th>\n",
       "      <th>event_reduction</th>\n",
       "      <th>Long_name</th>\n",
       "      <th>sampling</th>\n",
       "      <th>Program</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SWZ_D1</td>\n",
       "      <td>data/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>Fan_tweets_H_Sowoozoo_D1.csv</td>\n",
       "      <td>All_Tweets_SWZ_D1.csv</td>\n",
       "      <td>fan_Tweets_SWZ_D1.csv</td>\n",
       "      <td>fan_Tweets_SWZ_D1_reduced.csv</td>\n",
       "      <td>Setlists_sowoozoo_D1.csv</td>\n",
       "      <td>6MIN</td>\n",
       "      <td>[1, 2, 3, 6, 8, 9, 10, 12, 13, 15, 16, 19, 20,...</td>\n",
       "      <td>Sowoozoo Concert Day 1</td>\n",
       "      <td>#SOWOOZOO</td>\n",
       "      <td>SWZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SWZ_D2</td>\n",
       "      <td>data/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>Fan_tweets_H_Sowoozoo_D2.csv</td>\n",
       "      <td>All_Tweets_SWZ_D2.csv</td>\n",
       "      <td>fan_Tweets_SWZ_D2.csv</td>\n",
       "      <td>fan_Tweets_SWZ_D2_reduced.csv</td>\n",
       "      <td>Setlists_sowoozoo_D2.csv</td>\n",
       "      <td>108S</td>\n",
       "      <td>[1, 2, 3, 6, 8, 9, 10, 12, 13, 15, 16, 19, 20,...</td>\n",
       "      <td>Sowoozoo Concert Day 2</td>\n",
       "      <td>#SOWOOZOO</td>\n",
       "      <td>SWZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PTD_ON</td>\n",
       "      <td>data/PTD/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>FullPTD_Fan_tweets_PTD_ON_STAGE.csv</td>\n",
       "      <td>All_Tweets_PTD_ON.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON_reduced.csv</td>\n",
       "      <td>Setlists_PTD_ON.csv</td>\n",
       "      <td>25S</td>\n",
       "      <td>[1, 2, 3, 6, 7, 8, 9, 11, 12, 14, 15, 17, 18, ...</td>\n",
       "      <td>Permission to Dance on Stage</td>\n",
       "      <td>Kpop Stream</td>\n",
       "      <td>PTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PTD_LA4</td>\n",
       "      <td>data/PTD/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>PTD_LA4_Fan_tweets_FULLSTREAM.csv</td>\n",
       "      <td>All_Tweets_PTD_LA4.csv</td>\n",
       "      <td>fan_Tweets_PTD_LA4.csv</td>\n",
       "      <td>fan_Tweets_PTD_LA4_reduced.csv</td>\n",
       "      <td>Setlists_PTD_LA4.csv</td>\n",
       "      <td>40S</td>\n",
       "      <td>[1, 2, 3, 6, 7, 8, 9, 11, 12, 14, 15, 17, 18, ...</td>\n",
       "      <td>Permission to Dance LA Day 4</td>\n",
       "      <td>Kpop Stream</td>\n",
       "      <td>PTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PTD_ON_Alt1</td>\n",
       "      <td>data/PTD/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>Alt1PTD_Fan_tweets_PTD_ON_STAGE.csv</td>\n",
       "      <td>All_Tweets_PTD_ON_Alt1.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON_Alt1.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON_Alt1_reduced.csv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>Week prior to PTD On Stage</td>\n",
       "      <td>Kpop Stream</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PTD_ON_Alt2</td>\n",
       "      <td>data/PTD/</td>\n",
       "      <td>../StreamData/</td>\n",
       "      <td>./data/</td>\n",
       "      <td>Alt2PTD_Fan_tweets_PTD_ON_STAGE.csv</td>\n",
       "      <td>All_Tweets_PTD_ON_Alt2.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON_Alt2.csv</td>\n",
       "      <td>fan_Tweets_PTD_ON_Alt2_reduced.csv</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>Week following PTD On Stage</td>\n",
       "      <td>Kpop Stream</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tag    raw_loc  fullfeilds_loc  dep_loc  \\\n",
       "0       SWZ_D1      data/  ../StreamData/  ./data/   \n",
       "1       SWZ_D2      data/  ../StreamData/  ./data/   \n",
       "2       PTD_ON  data/PTD/  ../StreamData/  ./data/   \n",
       "3      PTD_LA4  data/PTD/  ../StreamData/  ./data/   \n",
       "4  PTD_ON_Alt1  data/PTD/  ../StreamData/  ./data/   \n",
       "5  PTD_ON_Alt2  data/PTD/  ../StreamData/  ./data/   \n",
       "\n",
       "                            raw_twt_db                 full_twt_db  \\\n",
       "0         Fan_tweets_H_Sowoozoo_D1.csv       All_Tweets_SWZ_D1.csv   \n",
       "1         Fan_tweets_H_Sowoozoo_D2.csv       All_Tweets_SWZ_D2.csv   \n",
       "2  FullPTD_Fan_tweets_PTD_ON_STAGE.csv       All_Tweets_PTD_ON.csv   \n",
       "3    PTD_LA4_Fan_tweets_FULLSTREAM.csv      All_Tweets_PTD_LA4.csv   \n",
       "4  Alt1PTD_Fan_tweets_PTD_ON_STAGE.csv  All_Tweets_PTD_ON_Alt1.csv   \n",
       "5  Alt2PTD_Fan_tweets_PTD_ON_STAGE.csv  All_Tweets_PTD_ON_Alt2.csv   \n",
       "\n",
       "                   fan_twt_db                          dep_twt_db  \\\n",
       "0       fan_Tweets_SWZ_D1.csv       fan_Tweets_SWZ_D1_reduced.csv   \n",
       "1       fan_Tweets_SWZ_D2.csv       fan_Tweets_SWZ_D2_reduced.csv   \n",
       "2       fan_Tweets_PTD_ON.csv       fan_Tweets_PTD_ON_reduced.csv   \n",
       "3      fan_Tweets_PTD_LA4.csv      fan_Tweets_PTD_LA4_reduced.csv   \n",
       "4  fan_Tweets_PTD_ON_Alt1.csv  fan_Tweets_PTD_ON_Alt1_reduced.csv   \n",
       "5  fan_Tweets_PTD_ON_Alt2.csv  fan_Tweets_PTD_ON_Alt2_reduced.csv   \n",
       "\n",
       "                 event_file event_offset  \\\n",
       "0  Setlists_sowoozoo_D1.csv         6MIN   \n",
       "1  Setlists_sowoozoo_D2.csv         108S   \n",
       "2       Setlists_PTD_ON.csv          25S   \n",
       "3      Setlists_PTD_LA4.csv          40S   \n",
       "4                                          \n",
       "5                                          \n",
       "\n",
       "                                     event_reduction  \\\n",
       "0  [1, 2, 3, 6, 8, 9, 10, 12, 13, 15, 16, 19, 20,...   \n",
       "1  [1, 2, 3, 6, 8, 9, 10, 12, 13, 15, 16, 19, 20,...   \n",
       "2  [1, 2, 3, 6, 7, 8, 9, 11, 12, 14, 15, 17, 18, ...   \n",
       "3  [1, 2, 3, 6, 7, 8, 9, 11, 12, 14, 15, 17, 18, ...   \n",
       "4                                                 []   \n",
       "5                                                 []   \n",
       "\n",
       "                      Long_name     sampling Program  \n",
       "0        Sowoozoo Concert Day 1    #SOWOOZOO     SWZ  \n",
       "1        Sowoozoo Concert Day 2    #SOWOOZOO     SWZ  \n",
       "2  Permission to Dance on Stage  Kpop Stream     PTD  \n",
       "3  Permission to Dance LA Day 4  Kpop Stream     PTD  \n",
       "4    Week prior to PTD On Stage  Kpop Stream          \n",
       "5   Week following PTD On Stage  Kpop Stream          "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Concerts = pd.DataFrame(columns=['tag','raw_loc','fullfeilds_loc','dep_loc',\n",
    "                                 'raw_twt_db','full_twt_db','fan_twt_db','dep_twt_db',\n",
    "                                 'event_file','event_offset','event_reduction','Long_name','sampling','Program'])\n",
    "Concerts.loc[0,:]={'tag': 'SWZ_D1','raw_loc':'data/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'Fan_tweets_H_Sowoozoo_D1.csv','full_twt_db':'All_Tweets_SWZ_D1.csv',\n",
    "             'fan_twt_db':'fan_Tweets_SWZ_D1.csv','dep_twt_db':'fan_Tweets_SWZ_D1_reduced.csv',\n",
    "             'event_file':'Setlists_sowoozoo_D1.csv',\n",
    "             'event_offset':'6MIN','event_reduction':[1,2,3,6,8,9,10,12,13,15,16,19,20,21,22,23,25,26,27,28],\n",
    "             'Long_name':'Sowoozoo Concert Day 1','sampling':'#SOWOOZOO','Program':'SWZ'}\n",
    "Concerts.loc[1,:]={'tag': 'SWZ_D2','raw_loc':'data/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'Fan_tweets_H_Sowoozoo_D2.csv','full_twt_db':'All_Tweets_SWZ_D2.csv',\n",
    "             'fan_twt_db':'fan_Tweets_SWZ_D2.csv','dep_twt_db':'fan_Tweets_SWZ_D2_reduced.csv',\n",
    "             'event_file':'Setlists_sowoozoo_D2.csv',\n",
    "             'event_offset':'108S','event_reduction':[1,2,3,6,8,9,10,12,13,15,16,19,20,21,22,23,25,26,27,28],\n",
    "             'Long_name':'Sowoozoo Concert Day 2','sampling':'#SOWOOZOO','Program':'SWZ'}\n",
    "Concerts.loc[2,:]={'tag': 'PTD_ON','raw_loc':'data/PTD/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'FullPTD_Fan_tweets_PTD_ON_STAGE.csv','full_twt_db':'All_Tweets_PTD_ON.csv',\n",
    "             'fan_twt_db':'fan_Tweets_PTD_ON.csv','dep_twt_db':'fan_Tweets_PTD_ON_reduced.csv',\n",
    "             'event_file':'Setlists_PTD_ON.csv','event_offset':'25S','event_reduction':[1,2,3,6,7,8,9,11,12,14,15,17,18,21,22,28,29,32,33,34,36,37,38,39],\n",
    "             'Long_name':'Permission to Dance on Stage','sampling':'Kpop Stream','Program':'PTD'}\n",
    "Concerts.loc[3,:]={'tag': 'PTD_LA4','raw_loc':'data/PTD/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'PTD_LA4_Fan_tweets_FULLSTREAM.csv','full_twt_db':'All_Tweets_PTD_LA4.csv',\n",
    "             'fan_twt_db':'fan_Tweets_PTD_LA4.csv','dep_twt_db':'fan_Tweets_PTD_LA4_reduced.csv',\n",
    "             'event_file':'Setlists_PTD_LA4.csv','event_offset':'40S','event_reduction':[1,2,3,6,7,8,9,11,12,14,15,17,18,21,22,28,29,32,33,34,36,37,38,39],\n",
    "             'Long_name':'Permission to Dance LA Day 4','sampling':'Kpop Stream','Program':'PTD'}\n",
    "Concerts.loc[4,:]={'tag': 'PTD_ON_Alt1','raw_loc':'data/PTD/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'Alt1PTD_Fan_tweets_PTD_ON_STAGE.csv','full_twt_db':'All_Tweets_PTD_ON_Alt1.csv',\n",
    "             'fan_twt_db':'fan_Tweets_PTD_ON_Alt1.csv','dep_twt_db':'fan_Tweets_PTD_ON_Alt1_reduced.csv',\n",
    "             'event_file':'','event_offset':'','event_reduction':[],\n",
    "             'Long_name':'Week prior to PTD On Stage','sampling':'Kpop Stream','Program':''}\n",
    "Concerts.loc[5,:]={'tag': 'PTD_ON_Alt2','raw_loc':'data/PTD/','fullfeilds_loc': '../StreamData/', 'dep_loc':'./data/',\n",
    "             'raw_twt_db':'Alt2PTD_Fan_tweets_PTD_ON_STAGE.csv','full_twt_db':'All_Tweets_PTD_ON_Alt2.csv',\n",
    "             'fan_twt_db':'fan_Tweets_PTD_ON_Alt2.csv','dep_twt_db':'fan_Tweets_PTD_ON_Alt2_reduced.csv',\n",
    "             'event_file':'','event_offset':'','event_reduction':[],\n",
    "             'Long_name':'Week following PTD On Stage','sampling':'Kpop Stream','Program':''}\n",
    "Concerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "# forced data types for csv files uploaded because pandas is acting up.\n",
    "\n",
    "dtype_map = {'id': 'Int64', 'created_at':str, 'tweet':str, 'source':str, 'language':str, 'user_id': 'Int64',\n",
    "       'user_screen_name':str, 'user_name':str, 'user_description':str, 'user_language':str,\n",
    "       'user_location':str, 'user_created_at':str, 'user_followers_count': 'Int64',\n",
    "       'user_friends_count': 'Int64', 'user_statuses_count': 'Int64', 'user_favorites_count': 'Int64',\n",
    "       'user_verified':str, 'in_reply_to_status_id': 'Int64', 'in_reply_to_user_id': 'Int64',\n",
    "       'in_reply_to_user_screen_name':str, 'retweeted_status_id': 'Int64',\n",
    "       'retweeted_status_user_id': 'Int64', 'retweeted_status_user_screen_name':str,\n",
    "       'retweeted_status_user_name':str, 'retweeted_status_user_description':str,\n",
    "       'retweeted_status_user_friends_count': 'Int64',\n",
    "       'retweeted_status_user_statuses_count': 'Int64',\n",
    "       'retweeted_status_user_followers_count': 'Int64',\n",
    "       'retweeted_status_retweet_count': 'Int64', 'retweeted_status_favorite_count': 'Int64',\n",
    "       'retweeted_status_reply_count': 'Int64', 'quoted_status_id': 'Int64',\n",
    "       'quoted_status_user_id': 'Int64', 'quoted_status_user_screen_name':str,\n",
    "       'quoted_status_user_name':str, 'quoted_status_user_description':str,\n",
    "       'quoted_status_user_friends_count': 'Int64', 'quoted_status_user_statuses_count': 'Int64',\n",
    "       'quoted_status_user_followers_count': 'Int64', 'quoted_status_retweet_count': 'Int64',\n",
    "       'quoted_status_favorite_count': 'Int64', 'quoted_status_reply_count': 'Int64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read an original recording of twitter api csv files with forced type and correct datetime handling\n",
    "df_alltwt=pd.read_csv(raw_dir + Concerts.loc[4,'raw_loc'] + Concerts.loc[4,'raw_twt_db']).astype(dtype_map)\n",
    "df_alltwt[\"created_at\"] = pd.to_datetime(df_alltwt[\"created_at\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out users per request\n",
    "\n",
    "Some twitter users explicitly refused consent to have their tweets cited or used for research purposes. While it is legally and technically easy to include their content, we have filtered out tweets by and retweets of users that include key phrases such as \"🚫 please do not cite my tweets w/o my express consent\" and \"💥This acct DOES NOT consent to being used for research purposes 💥\"  in their users bio texts. \n",
    "\n",
    "The exclusion list was built by reviewing instances of key words in English and Korean to find representative phrases. Most common were instances of artists specifying restrictions on reposting or reusing the media shared in their tweets. As the Streaming API does not collect media (images videos), this was not a reason to exclude these accounts. Accounts identified as requesting exclusion mostly used forms of \"DON'T USE\" and mentions of consent. Examples of this search process can found in notebook Tweet_content_review.ipynb. This removed 107 entries from 12 in one concert dataset. \n",
    "\n",
    "This was not an exhaustive review of user consent to be included in research. Iterative keyword search is a limited strategy, thematically and crosslinguistically. However, the low number of these cases captured suggests these were not a concentrated concern. Additionally, our use and sharing of the depersonalised datasets should not pose any risk to such users. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample search keys for 'user_description' field of tweets\n",
    "\n",
    "kw ='무단사용 및 도용금지'\n",
    "['DOES NOT',\"DON'T USE\", 'research', 'credit', 'repost', 'scrap', 'media', 'steal', 'publication', 'permission', 'approve', 'approval', 'permit', 'collect', 'cite', 'quote']\n",
    "\n",
    "내 명시적인 동의 없이 트윗을 사용하지 마세요, 인용하지 마십시오, 허락을 🚫 무단사용 및 도용금지 and roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output for databases after clearing out accounts identified for exclusion.\n",
    "# this folder location is outside of this repo, as these databases are not classified as green data (open sharable)\n",
    "data_loc = '../Stream_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dontuse_users = pd.read_csv(data_loc+'Exclude_accounts.csv',header = None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dontuse_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWZ_D1\n",
      "Size of full set: 225993\n",
      "Size of cleaned set: 225934\n",
      "SWZ_D2\n",
      "Size of full set: 114728\n",
      "Size of cleaned set: 114724\n",
      "PTD_ON\n",
      "Size of full set: 277794\n",
      "Size of cleaned set: 277794\n",
      "PTD_LA4\n",
      "Size of full set: 143837\n",
      "Size of cleaned set: 143772\n",
      "PTD_ON_Alt1\n",
      "Size of full set: 16806\n",
      "Size of cleaned set: 16802\n",
      "PTD_ON_Alt2\n",
      "Size of full set: 55676\n",
      "Size of cleaned set: 55671\n"
     ]
    }
   ],
   "source": [
    "for i in Concerts.index:\n",
    "    df_alltwt=pd.read_csv(raw_dir + Concerts.loc[i,'raw_loc'] + Concerts.loc[i,'raw_twt_db'],\n",
    "                 lineterminator='\\n',low_memory=False)#.astype(dtype_map)\n",
    "    df_alltwt[\"created_at\"] = pd.to_datetime(df_alltwt[\"created_at\"])\n",
    "    data_name = Concerts.loc[i,'tag']\n",
    "    print(data_name)\n",
    "    # Clean up\n",
    "    print('Size of full set: ' + str(len(df_alltwt)))\n",
    "    \n",
    "    df_fantwt = df_alltwt.copy()\n",
    "    # remove tweets from or connected to identified users by user_id\n",
    "    for acc in dontuse_users:\n",
    "        df_fantwt = df_fantwt.loc[df_fantwt['user_id']!=acc,:].copy()\n",
    "        df_fantwt = df_fantwt.loc[df_fantwt['retweeted_status_user_id']!=acc,:].copy()\n",
    "        df_fantwt = df_fantwt.loc[df_fantwt['in_reply_to_user_id']!=acc,:].copy()\n",
    "        df_fantwt = df_fantwt.loc[df_fantwt['quoted_status_user_id']!=acc,:].copy()\n",
    "\n",
    "    print('Size of cleaned set: ' + str(len(df_fantwt)))\n",
    "    df_alltwt = df_fantwt.sort_values('created_at').reset_index(drop=True)\n",
    "    df_alltwt.to_csv(Concerts.loc[i,'fullfeilds_loc'] + Concerts.loc[i,'full_twt_db'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concerts = pd.DataFrame(columns=['tag','data_loc','twt_dbs','event_file','event_offset','event_reduction','Long_name','sampling','Program'])\n",
    "Concerts.loc[0,:]={'tag': 'SWZ_D1','data_loc':'data/',\n",
    "             'twt_dbs':'All_Tweets_SWZ_D1.csv','event_file':'Setlists_sowoozoo_D1.csv','event_offset':'6MIN','event_reduction':[1,2,3,6,8,9,10,12,13,15,16,19,20,21,22,23,25,26,27,28],\n",
    "             'Long_name':'Sowoozoo Concert Day 1','sampling':'#SOWOOZOO','Program':'SWZ'}\n",
    "Concerts.loc[1,:]={'tag': 'SWZ_D2','data_loc':'data/',\n",
    "             'twt_dbs':'All_Tweets_SWZ_D2.csv','event_file':'Setlists_sowoozoo_D2.csv','event_offset':'108S','event_reduction':[1,2,3,6,8,9,10,12,13,15,16,19,20,21,22,23,25,26,27,28],\n",
    "             'Long_name':'Sowoozoo Concert Day 2','sampling':'#SOWOOZOO','Program':'SWZ'}\n",
    "Concerts.loc[2,:]={'tag': 'PTD_ON','data_loc':'data/PTD/',\n",
    "             'twt_dbs':'All_Tweets_PTD_ON.csv','event_file':'Setlists_PTD_ON.csv','event_offset':'25S','event_reduction':[1,2,3,6,7,8,9,11,12,14,15,17,18,21,22,28,29,32,33,34,36,37,38,39],\n",
    "             'Long_name':'Permission to Dance on Stage','sampling':'Kpop Stream','Program':'PTD'}\n",
    "Concerts.loc[3,:]={'tag': 'PTD_LA4','data_loc':'data/PTD/',\n",
    "             'twt_dbs':'All_Tweets_PTD_LA4.csv','event_file':'Setlists_PTD_LA4.csv','event_offset':'40S','event_reduction':[1,2,3,6,7,8,9,11,12,14,15,17,18,21,22,28,29,32,33,34,36,37,38,39],\n",
    "             'Long_name':'Permission to Dance LA Day 4','sampling':'Kpop Stream','Program':'PTD'}\n",
    "Concerts.loc[4,:]={'tag': 'PTD_ON_Alt1','data_loc':'data/PTD/',\n",
    "             'twt_dbs':'All_Tweets_PTD_ON_Alt1.csv','event_file':'','event_offset':'','event_reduction':[],\n",
    "             'Long_name':'Week prior to PTD On Stage','sampling':'Kpop Stream','Program':''}\n",
    "Concerts.loc[5,:]={'tag': 'PTD_ON_Alt2','data_loc':'data/PTD/',\n",
    "             'twt_dbs':'All_Tweets_PTD_ON_Alt2.csv','event_file':'','event_offset':'','event_reduction':[],\n",
    "             'Long_name':'Week following PTD On Stage','sampling':'Kpop Stream','Program':''}\n",
    "Concerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-fan content\n",
    "Tweets from or to official accounts are in these datasets, both hashtag and stream samples, but are not of interest to this research question. We are not interested in tweets to the artists or from the the production company, these have different dynamics. \n",
    "\n",
    "Our list of exclusion: '@BTS_twt', '@bts_bighit', '@weverseofficial', '@weverseshop', '@BIGHIT_MUSIC', '@HYBE_MERCH', '@BT21_'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_users = ['@BTS_twt','@bts_bighit','@weverseofficial','@weverseshop','@BIGHIT_MUSIC','@HYBE_MERCH','@BT21_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWZ_D1\n",
      "Size of full set: 225934\n",
      "Size of reply tweets: 1730\n",
      "Size of replys: 1484\n",
      "Size of cleaned set: 224661\n",
      "SWZ_D2\n",
      "Size of full set: 114724\n",
      "Size of reply tweets: 1303\n",
      "Size of replys: 947\n",
      "Size of cleaned set: 111152\n",
      "PTD_ON\n",
      "Size of full set: 277794\n",
      "Size of reply tweets: 56674\n",
      "Size of replys: 8213\n",
      "Size of cleaned set: 228708\n",
      "PTD_LA4\n",
      "Size of full set: 143772\n",
      "Size of reply tweets: 28387\n",
      "Size of replys: 4069\n",
      "Size of cleaned set: 116313\n",
      "PTD_ON_Alt1\n",
      "Size of full set: 16802\n",
      "Size of reply tweets: 3605\n",
      "Size of replys: 1235\n",
      "Size of cleaned set: 14269\n",
      "PTD_ON_Alt2\n",
      "Size of full set: 55671\n",
      "Size of reply tweets: 11997\n",
      "Size of replys: 3444\n",
      "Size of cleaned set: 46790\n"
     ]
    }
   ],
   "source": [
    "for i in Concerts.index:\n",
    "    df_alltwt=pd.read_csv(Concerts.loc[i,'fullfeilds_loc'] + Concerts.loc[i,'full_twt_db'],\n",
    "                 lineterminator='\\n',index_col = 0,low_memory=False).astype(dtype_map).reset_index(drop = True)\n",
    "    df_alltwt[\"created_at\"] = pd.to_datetime(df_alltwt[\"created_at\"])\n",
    "    \n",
    "    data_name = Concerts.loc[i,'tag']\n",
    "    print(data_name)\n",
    "    # Clean up\n",
    "    print('Size of full set: ' + str(len(df_alltwt)))\n",
    "    df_fantwt = df_alltwt.copy()\n",
    "    # quote or retweets of accounts by user\n",
    "    for acc in filtered_users:\n",
    "        twts = df_fantwt['tweet'] \n",
    "        df_fantwt = df_fantwt.loc[~(twts.str.contains('T '+acc+':', case=False,regex=False))].copy()\n",
    "    # actually from these users\n",
    "    for acc in filtered_users:\n",
    "        twts = df_fantwt['user_screen_name'] \n",
    "        df_fantwt = df_fantwt.loc[~(twts.str.contains(acc[1:], case=False,regex=False))].copy()\n",
    "\n",
    "    #removing replys to these accounts\n",
    "    df_notreplys = df_fantwt.loc[df_fantwt['in_reply_to_user_id'].isna(),:]\n",
    "    df_replys = df_fantwt.loc[df_fantwt['in_reply_to_user_id'].notna(),:]\n",
    "    print('Size of reply tweets: ' + str(len(df_replys)) )\n",
    "    twts = df_replys['in_reply_to_user_screen_name']  # df_replys['in_reply_to_user_screen_name'].value_counts()\n",
    "    for acc in filtered_users: # filter RTs and replys\n",
    "         df_replys =  df_replys.loc[~(twts.str.startswith(acc[1:]))]\n",
    "    print('Size of replys: ' + str(len(df_replys)) )\n",
    "    \n",
    "    df_alltwt = pd.concat([df_notreplys,df_replys])\n",
    "    print('Size of cleaned set: ' + str(len(df_alltwt)))\n",
    "    \n",
    "    df_alltwt = df_alltwt.sort_values('created_at').reset_index(drop=True)\n",
    "    \n",
    "    df_alltwt.to_csv(Concerts.loc[i,'fullfeilds_loc'] + Concerts.loc[i,'fan_twt_db'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction 1: Depersonalised full tweet sets\n",
    "\n",
    "Once the databases have been filtered for exclusions and official account related activity, we go through with depersonalising the initial set of information saved per tweet to fields that are sufficiently depersonalised. \n",
    "\n",
    "Unique identifier numbers are replaced by hashtable per dataset, both tweet ids and user ids. Relevant tweet and users numerical statistics are retained as they are non-searchable and non-unique descriptors of posts and accounts from a non-retreivable time in the twitter database's history. Tweet text, user names, and user descriptions are potentially unique and identifiable and thus removed. \n",
    "\n",
    "List of fields retained (without alternation):\n",
    "\n",
    "    - 'created_at'\n",
    "    \n",
    "    - 'retweeted_status_retweet_count'\n",
    "    - 'retweeted_status_favorite_count'\n",
    "    - 'retweeted_status_reply_count'\n",
    "    - 'quoted_status_retweet_count'\n",
    "    - 'quoted_status_favorite_count'\n",
    "    - 'quoted_status_reply_count'\n",
    "    \n",
    "    - 'user_followers_count'\n",
    "    - 'retweeted_status_user_followers_count'\n",
    "    - 'quoted_status_user_followers_count'\n",
    "\n",
    "Info to replace with hashtable:\n",
    "    \n",
    "    - 'id'\n",
    "\t- 'user_id'\n",
    "    - 'in_reply_to_status_id'\n",
    "    - 'in_reply_to_user_id'\n",
    "    - 'retweeted_status_id'\n",
    "    - 'retweeted_status_user_id'\n",
    "    - 'quoted_status_id'\n",
    "    - 'quoted_status_user_id'\n",
    "    \n",
    "    \n",
    "Features extrated from tweet content:\n",
    "\n",
    "\t- Tweet length in characters\n",
    "\t- Media inclusion (embedded photo, video, or quoting another tweet)\n",
    "    - Tweet type (Original, RT, QT, Reply)\n",
    "    \n",
    "List of fields discarded entirely:\n",
    "\n",
    "    - 'tweet' \n",
    "    - 'source'\n",
    "    - 'language'\n",
    "    - 'user_screen_name'\n",
    "    - 'user_name'\n",
    "    - 'user_description'\n",
    "    - 'user_language'\n",
    "    - 'user_location'\n",
    "    - 'user_created_at'\n",
    "    - 'user_friends_count'\n",
    "    - 'user_statuses_count'\n",
    "    - 'user_favorites_count'\n",
    "    - 'user_verified'\n",
    "    \n",
    "    - 'in_reply_to_user_screen_name'\n",
    "    \n",
    "    - 'retweeted_status_user_screen_name'\n",
    "    - 'retweeted_status_user_name'\n",
    "    - 'retweeted_status_user_description'\n",
    "    - 'retweeted_status_user_friends_count'\n",
    "    - 'retweeted_status_user_statuses_count'\n",
    "    \n",
    "    - 'quoted_status_user_screen_name'\n",
    "    - 'quoted_status_user_name'\n",
    "    - 'quoted_status_user_description'\n",
    "    - 'quoted_status_user_friends_count'\n",
    "    - 'quoted_status_user_statuses_count'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWZ_D1\n"
     ]
    }
   ],
   "source": [
    "df_alltwt=pd.read_csv(Concerts.loc[i,'fullfeilds_loc'] + Concerts.loc[i,'fan_twt_db'],\n",
    "             lineterminator='\\n',index_col = 0)#.astype(dtype_map)\n",
    "data_name = Concerts.loc[i,'tag']\n",
    "print(data_name)\n",
    "\n",
    "df_alltwt[\"created_at\"] = pd.to_datetime(df_alltwt[\"created_at\"])\n",
    "df_alltwt = df_alltwt.sort_values('created_at').reset_index(drop=True)\n",
    "\n",
    "Feilds_to_keep = ['created_at','id','user_id','user_followers_count',\n",
    "'retweeted_status_id','retweeted_status_user_id','retweeted_status_user_followers_count',\n",
    "'retweeted_status_retweet_count', 'retweeted_status_favorite_count','retweeted_status_reply_count',\n",
    "'quoted_status_id','quoted_status_user_id','quoted_status_user_followers_count',\n",
    "'quoted_status_retweet_count','quoted_status_favorite_count','quoted_status_reply_count',\n",
    "'in_reply_to_status_id','in_reply_to_user_id']\n",
    "\n",
    "df_Reduced = df_alltwt.loc[:,Feilds_to_keep]#.astype('Int64')\n",
    "twts = df_alltwt['tweet']\n",
    "dates = df_alltwt['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                               datetime64[ns, UTC]\n",
       "id                                                     int64\n",
       "user_id                                                int64\n",
       "user_followers_count                                   int64\n",
       "retweeted_status_id                                  float64\n",
       "retweeted_status_user_id                             float64\n",
       "retweeted_status_user_followers_count                float64\n",
       "retweeted_status_retweet_count                       float64\n",
       "retweeted_status_favorite_count                      float64\n",
       "retweeted_status_reply_count                         float64\n",
       "quoted_status_id                                     float64\n",
       "quoted_status_user_id                                float64\n",
       "quoted_status_user_followers_count                   float64\n",
       "quoted_status_retweet_count                          float64\n",
       "quoted_status_favorite_count                         float64\n",
       "quoted_status_reply_count                            float64\n",
       "in_reply_to_status_id                                float64\n",
       "in_reply_to_user_id                                  float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Reduced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set ids', 0.24439120292663574]\n",
      "['create hash', 0.4201631546020508]\n",
      "['user_id', 85.2270519733429]\n",
      "['in_reply_to_user_id', 146.00992608070374]\n",
      "['retweeted_status_user_id', 207.81203508377075]\n",
      "['quoted_status_user_id', 268.20725297927856]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hash the user ids across all user id feilds.\n",
    "Feilds_to_anon = ['user_id','in_reply_to_user_id','retweeted_status_user_id','quoted_status_user_id']\n",
    "ids = []\n",
    "tic = time.time()\n",
    "for replace_feild in Feilds_to_anon:\n",
    "    ids +=list(df_alltwt.loc[df_alltwt[replace_feild].notna(),replace_feild].astype('int64').values)\n",
    "ids=pd.DataFrame(columns = ['index'],data = ids)\n",
    "print(['set ids',time.time()-tic])\n",
    "A = ids.value_counts().reset_index()\n",
    "B = dict((v, k) for k, v in A['index'].to_dict().items())\n",
    "print(['create hash',time.time()-tic])\n",
    "for replace_feild in Feilds_to_anon:\n",
    "    df_Reduced[replace_feild].replace(B,inplace = True)\n",
    "    print([replace_feild,time.time()-tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['set ids', 0.28423118591308594]\n",
      "['create hash', 0.5798389911651611]\n",
      "['id', 183.05087304115295]\n",
      "['in_reply_to_status_id', 319.56561303138733]\n",
      "['retweeted_status_id', 456.2954912185669]\n",
      "['quoted_status_id', 590.9220142364502]\n"
     ]
    }
   ],
   "source": [
    "# Hash the tweet ids across all tweet id feilds.\n",
    "Feilds_to_anon = ['id','in_reply_to_status_id', 'retweeted_status_id', 'quoted_status_id']\n",
    "ids = []\n",
    "tic = time.time()\n",
    "for replace_feild in Feilds_to_anon:\n",
    "    ids +=list(df_alltwt.loc[df_alltwt[replace_feild].notna(),replace_feild].astype('int64').values)\n",
    "ids=pd.DataFrame(columns = ['index'],data = ids)\n",
    "print(['set ids',time.time()-tic])\n",
    "A = ids.value_counts().reset_index()\n",
    "B = dict((v, k) for k, v in A['index'].to_dict().items())\n",
    "print(['create hash',time.time()-tic])\n",
    "for replace_feild in Feilds_to_anon:\n",
    "    df_Reduced[replace_feild].replace(B,inplace = True)\n",
    "    print([replace_feild,time.time()-tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWZ_D1\n"
     ]
    }
   ],
   "source": [
    "# add in binary tweet type categories: RT, QT, Reply, Original\n",
    "df_Reduced['Original'] = 1\n",
    "df_Reduced.loc[df_alltwt['retweeted_status_id'].notna(),'Original'] = 0\n",
    "df_Reduced.loc[df_alltwt['quoted_status_id'].notna(),'Original'] = 0\n",
    "df_Reduced.loc[df_alltwt['in_reply_to_status_id'].notna(),'Original'] = 0\n",
    "df_Reduced['RT'] = 0\n",
    "df_Reduced.loc[df_alltwt['retweeted_status_id'].notna(),'RT'] = 1\n",
    "df_Reduced['QT'] = 0\n",
    "df_Reduced.loc[df_alltwt['quoted_status_id'].notna(),'QT'] = 1\n",
    "df_Reduced['Reply'] = 0\n",
    "df_Reduced.loc[df_alltwt['in_reply_to_status_id'].notna(),'Reply'] = 1\n",
    "\n",
    "# add features of tweet content\n",
    "df_Reduced['Media'] = 0\n",
    "df_Reduced.loc[twts.str.contains('https://t.co', case=False,regex=False),'Media'] = 1\n",
    "df_Reduced['Length'] = twts.str.len()\n",
    "\n",
    "df_Reduced.to_csv(Concerts.loc[i,'dep_loc'] + Concerts.loc[i,'dep_twt_db'])\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on coded subsets\n",
    "Clean up coded subset for easy processing\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RT_SubSet', 'Concert', 'Last_RT_created_at', 'Last_RT_id',\n",
       "       'Last_RT_url', 'OriTwt_id', 'language', 'LastRT_user_followers_count',\n",
       "       'Last_RT_user_friends_count', 'Last_RT_user_statuses_count',\n",
       "       'Last_RT_user_favorites_count', 'Ori_Twt_user_friends_count',\n",
       "       'Ori_Twt_user_statuses_count', 'Ori_Twt_user_followers_count',\n",
       "       'Last_RT_retweet_count', 'Last_RT_favorite_count',\n",
       "       'Last_RT_reply_count', 'Shout', 'Tweet Length', 'Tweet link/media',\n",
       "       'Lost', 'Unrelated', 'Affection', 'Intensifiers', 'Self', 'Members',\n",
       "       'recording', 'Stills', 'Production', 'Music', 'Commentary', 'Army',\n",
       "       'Anticipation', 'Fanwork', 'Information', 'Stream', 'Spam',\n",
       "       'Commercial'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloc='/Users/finn/Desktop/Current_Projects/BTS_twitter/twt_Analysis/data/'\n",
    "subTwts = pd.read_csv(dloc + 'Packed_PTD_Subsets_Coded.csv')\n",
    "\n",
    "data_name = subTwts.loc[0,'Concert']\n",
    "subTwts['Last_RT_created_at'] = pd.to_datetime(subTwts['Last_RT_created_at'])\n",
    "subTwts['Last_RT_id'] = pd.to_datetime(subTwts['Last_RT_created_at'])\n",
    "subTwts['Last_RT_id'] = subTwts['Last_RT_id'].astype('int64')\n",
    "# convert codes to boolean or binary. binary? \n",
    "codes = ['Affection', 'Intensifiers', 'Self', 'Members', 'recording', 'Stills',\n",
    "       'Production', 'Music', 'Commentary', 'Army', 'Anticipation', 'Fanwork',\n",
    "       'Information', 'Stream', 'Spam', 'Commercial']\n",
    "for c in codes:\n",
    "    subTwts.loc[subTwts[c].notna(),c] = 1\n",
    "    subTwts.loc[subTwts[c].isna(),c] = 0\n",
    "    subTwts[c] = subTwts[c].astype('bool')\n",
    "\n",
    "subTwts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT_SubSet unique 4 object\n",
      "['Top200RTd' 'Rand200_32t6RTd' 'Rand200_3t1RTd' 'Rand200_NoRT']\n",
      "Concert unique 1 object\n",
      "['PTD_ON1']\n",
      "Last_RT_created_at unique 468 datetime64[ns, UTC]\n",
      "Last_RT_id unique 468 int64\n",
      "Last_RT_url unique 484 object\n",
      "OriTwt_id unique 484 int64\n",
      "language unique 24 object\n",
      "LastRT_user_followers_count unique 287 int64\n",
      "Last_RT_user_friends_count unique 314 int64\n",
      "Last_RT_user_statuses_count unique 483 int64\n",
      "Last_RT_user_favorites_count unique 469 int64\n",
      "Ori_Twt_user_friends_count unique 340 int64\n",
      "Ori_Twt_user_statuses_count unique 454 int64\n",
      "Ori_Twt_user_followers_count unique 447 int64\n",
      "Last_RT_retweet_count unique 149 int64\n",
      "Last_RT_favorite_count unique 210 int64\n",
      "Last_RT_reply_count unique 98 int64\n",
      "Shout unique 2 bool\n",
      "[False  True]\n",
      "Tweet Length unique 170 int64\n",
      "Tweet link/media unique 2 bool\n",
      "[ True False]\n",
      "Lost unique 1 float64\n",
      "[nan  1.]\n",
      "Unrelated unique 1 float64\n",
      "[ 1. nan]\n",
      "Affection unique 2 bool\n",
      "[False  True]\n",
      "Intensifiers unique 2 bool\n",
      "[False  True]\n",
      "Self unique 2 bool\n",
      "[False  True]\n",
      "Members unique 2 bool\n",
      "[False  True]\n",
      "recording unique 2 bool\n",
      "[False  True]\n",
      "Stills unique 2 bool\n",
      "[False  True]\n",
      "Production unique 2 bool\n",
      "[False  True]\n",
      "Music unique 2 bool\n",
      "[False  True]\n",
      "Commentary unique 2 bool\n",
      "[False  True]\n",
      "Army unique 2 bool\n",
      "[False  True]\n",
      "Anticipation unique 2 bool\n",
      "[False  True]\n",
      "Fanwork unique 2 bool\n",
      "[False  True]\n",
      "Information unique 2 bool\n",
      "[False  True]\n",
      "Stream unique 2 bool\n",
      "[False  True]\n",
      "Spam unique 1 bool\n",
      "[False]\n",
      "Commercial unique 1 bool\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "# check formating\n",
    "for col in subTwts.columns:\n",
    "    N = subTwts[col].nunique()\n",
    "    print(' '.join([col,'unique',str(N), str(subTwts[col].dtypes)]))\n",
    "    if N<10:\n",
    "        print(subTwts[col].unique())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " subTwts.to_csv('./data/'+data_name+'_coded_subsets_with_replaced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
